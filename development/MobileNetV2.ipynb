{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "clear-marker",
   "metadata": {},
   "source": [
    "# MobileNetV2 in XABY\n",
    "\n",
    "In this notebook I'm going to implement the MobileNetV2 model in XABY. Read the original paper here: https://arxiv.org/abs/1801.04381. I'm following the paper as well as the [torchvision implementation](https://github.com/pytorch/vision/blob/master/torchvision/models/mobilenetv2.py).\n",
    "\n",
    "The goal here is to add features to XABY that are needed to for the MobileNetV2 model. My typically workflow is to develop new things in a notebook like this, then copy it over to the actual module files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Union\n",
    "\n",
    "import xaby as xb\n",
    "import xaby.nn as xn\n",
    "from xaby import jnp\n",
    "\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax.ops import index, index_add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-carter",
   "metadata": {},
   "source": [
    "The original paper uses ReLU6 activations, implement that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "@xb.fn\n",
    "def relu6(x: jnp.DeviceArray) -> jnp.DeviceArray:\n",
    "    return jnp.clip(x, a_min=0, a_max=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-petersburg",
   "metadata": {},
   "source": [
    "The MobileNetV2 model uses [batch normalization](https://arxiv.org/abs/1502.03167) (known as batchnorm). As of starting this notebook, batchnorm doesn't exist in XABY. So, implement it here.\n",
    "\n",
    "Typically, during evaluation batch normalization is modified to use global statistics (mean and variance) instead of the batch statistics. You would implement this by tracking the mean and variance of every batch you used to train in the batchnorm layer. But, this requires modifying state resulting in an impure function. To get JAX's jit to work, you can use only pure functions. I haven't found a good way around this yet, so this batchnorm will continue to use batch statistics at evaluation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "class batchnorm2d(xb.Fn):\n",
    "    def __init__(self, num_features: int, epsilon: int=1e-5):\n",
    "        \"\"\" BatchNorm for 2D input, expects 4D input array in (B, C, H, W) format \"\"\"\n",
    "        \n",
    "        @jax.jit\n",
    "        def batchnorm2d(inputs: xb.ArrayList, params: dict) -> xb.ArrayList:\n",
    "            x, = inputs\n",
    "            weights, bias = params[\"weights\"], params[\"bias\"]\n",
    "            num_features = x.shape[1]\n",
    "            \n",
    "            # Reshaping for broadcasting ease\n",
    "            x_mean = jnp.mean(x, axis=(0, 2, 3)).reshape(1, num_features, 1, 1)\n",
    "            x_var = jnp.mean((x - x_mean)**2, axis=(0, 2, 3)).reshape(1, num_features, 1, 1)\n",
    "                    \n",
    "            x_norm = (x - x_mean) / jnp.sqrt(x_var + epsilon)\n",
    "            y = weights * x_norm + bias\n",
    "            return xb.pack(y)\n",
    "        \n",
    "        super().__init__(batchnorm2d, 1, 1, name=\"batchnorm2d\")\n",
    "        \n",
    "        self.params[\"weights\"] = jnp.ones((1, num_features, 1, 1))\n",
    "        self.params[\"bias\"] = jnp.zeros((1, num_features, 1, 1))\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"batchnorm({self.num_features}, epsilon={self.epsilon})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-respondent",
   "metadata": {},
   "source": [
    "Here I'll create functions for creating sub-functions of the overall model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(func: xb.Fn) -> xb.Fn:\n",
    "    return xb.parallel(func, xb.skip) >> xb.add\n",
    "\n",
    "def conv_norm(in_f: int, out_f: int, kernel_size: int = 3, stride: int = 1, padding:int = 0, groups: int = 1, \n",
    "              norm: Optional[xb.Fn] = None, activation: Optional[xb.Fn] = None):\n",
    "    \"\"\" Returns a function conv2d >> batchnorm2d >> relu6 by default \"\"\"\n",
    "    \n",
    "    if norm is None:\n",
    "        norm = batchnorm2d\n",
    "    if activation is None:\n",
    "        activation = relu6\n",
    "        \n",
    "    conv = xn.conv2d(in_f, out_f, kernel_size, stride, padding, groups, bias=False)\n",
    "    func = conv >> norm(out_f) >> activation\n",
    "    func.name = \"conv_norm\"\n",
    "    return func\n",
    "\n",
    "def bottleneck(in_features: int, out_features: int, stride: int=1, expand_ratio: Union[int, float] = 1.0) -> xb.Fn:\n",
    "    \"\"\" Returns an Inverted Residual Bottleneck function \"\"\"\n",
    "    \n",
    "    expand_f = int(round(expand_ratio * in_features))\n",
    "    \n",
    "    if expand_ratio != 1:\n",
    "        expand = conv_norm(in_features, expand_f, kernel_size=1)\n",
    "    else:\n",
    "        expand = None\n",
    "    \n",
    "    depthwise = conv_norm(expand_f, expand_f, stride=stride, padding=1, groups=expand_f)\n",
    "    \n",
    "    transform = xn.conv2d(expand_f, out_features, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "    \n",
    "    if expand is not None:\n",
    "        func = expand >> depthwise >> transform >> batchnorm2d(out_features)\n",
    "    else:\n",
    "        func = depthwise >> transform >> batchnorm2d(out_features)\n",
    "    \n",
    "    if in_features == out_features and stride == 1:\n",
    "        func = residual(func)\n",
    "        \n",
    "    func.name = \"bottleneck\"\n",
    "    \n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-supervision",
   "metadata": {},
   "source": [
    "Time to put the parts together into one big function / model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-yeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "    \n",
    "def mobilenetv2(num_classes: int =1000, width_mult: float = 1.0, round_nearest=1, architecture: Optional[List[List[int]]]=None) -> xb.Fn:\n",
    "    \n",
    "    in_features = 32\n",
    "    last_features = 1280\n",
    "    \n",
    "    # Architecture from the MobileNetV2 paper\n",
    "    if architecture is None:\n",
    "        architecture = [\n",
    "            # t, c, n, s\n",
    "            [1, 16, 1, 1],\n",
    "            [6, 24, 2, 2],\n",
    "            [6, 32, 3, 2],\n",
    "            [6, 64, 4, 2],\n",
    "            [6, 96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "        \n",
    "    in_features = _make_divisible(in_features * width_mult, round_nearest)\n",
    "    last_features = _make_divisible(last_features * max(1.0, width_mult), round_nearest)\n",
    "    \n",
    "    features = xn.conv2d(3, in_features, stride=2, padding=1) >> batchnorm2d(in_features)\n",
    "    \n",
    "    # Add bottleneck layers\n",
    "    for t, c, n, s in architecture:\n",
    "        for i in range(n):\n",
    "            out_features = c\n",
    "            # We only change the dimensions at the first bottleneck layer of any series\n",
    "            stride = s if i == 0 else 1\n",
    "            \n",
    "            features >> bottleneck(in_features, out_features, stride=stride, expand_ratio=t)\n",
    "            \n",
    "            in_features = out_features\n",
    "\n",
    "    features = features >> conv_norm(in_features, last_features, kernel_size=1)\n",
    "    features.name = \"features\"\n",
    "    \n",
    "    classifier = xn.dropout(0.2) >> xn.linear(last_features, num_classes)\n",
    "    classifier.name = \"classifier\"\n",
    "    \n",
    "    # The mean here computes the spatial average of each feature. This is sometimes known as \n",
    "    # global average pooling. It's becoming more popular in new models. I like it because it\n",
    "    # avoids imposing a fixed input size for the model. You can train the model on 224x224 images \n",
    "    # like normal, but then use the model for images with other sizes and it still works.\n",
    "    model = features >> xb.mean(axis=(2, 3)) >> classifier\n",
    "    model.name = \"mobilenetv2\"\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-memorial",
   "metadata": {},
   "source": [
    "Now we can create the model itself and look at the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "double-effort",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential('mobilenetv2') {\n",
       "  (features): sequential('features') {\n",
       "    (conv2d): conv2d(3, 32, kernel=(3, 3), stride=(2, 2), padding=((1, 1), (1, 1)), bias=True)\n",
       "    (batchnorm2d): batchnorm(32, epsilon=1e-05)\n",
       "    (bottleneck): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(32, 32, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(32, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(32, 16, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(16, epsilon=1e-05)\n",
       "    }\n",
       "    (bottleneck_1): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(16, 96, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "        (batchnorm2d): batchnorm(96, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv_norm_1): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(96, 96, kernel=(3, 3), stride=(2, 2), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(96, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(96, 24, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(24, epsilon=1e-05)\n",
       "    }\n",
       "    (bottleneck_2): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(24, 144, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(144, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(144, 144, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(144, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(144, 24, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(24, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_3): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(24, 144, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "        (batchnorm2d): batchnorm(144, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv_norm_1): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(144, 144, kernel=(3, 3), stride=(2, 2), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(144, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(144, 32, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(32, epsilon=1e-05)\n",
       "    }\n",
       "    (bottleneck_4): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(32, 192, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(192, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(192, 192, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(192, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(192, 32, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(32, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_5): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(32, 192, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(192, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(192, 192, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(192, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(192, 32, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(32, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_6): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(32, 192, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "        (batchnorm2d): batchnorm(192, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv_norm_1): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(192, 192, kernel=(3, 3), stride=(2, 2), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(192, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(192, 64, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(64, epsilon=1e-05)\n",
       "    }\n",
       "    (bottleneck_7): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(64, 384, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(384, 384, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(384, 64, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(64, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_8): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(64, 384, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(384, 384, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(384, 64, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(64, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_9): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(64, 384, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(384, 384, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(384, 64, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(64, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_10): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(64, 384, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "        (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv_norm_1): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(384, 384, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(384, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(384, 96, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(96, epsilon=1e-05)\n",
       "    }\n",
       "    (bottleneck_11): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(96, 576, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(576, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(576, 576, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(576, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(576, 96, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(96, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_12): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(96, 576, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(576, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(576, 576, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(576, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(576, 96, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(96, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_13): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(96, 576, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "        (batchnorm2d): batchnorm(576, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv_norm_1): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(576, 576, kernel=(3, 3), stride=(2, 2), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(576, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(576, 160, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(160, epsilon=1e-05)\n",
       "    }\n",
       "    (bottleneck_14): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(160, 960, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(960, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(960, 960, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(960, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(960, 160, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(160, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_15): sequential('bottleneck') {\n",
       "      (parallel): parallel('parallel') {\n",
       "        (sequential): sequential('sequential') {\n",
       "          (conv_norm): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(160, 960, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "            (batchnorm2d): batchnorm(960, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv_norm_1): sequential('conv_norm') {\n",
       "            (conv2d): conv2d(960, 960, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "            (batchnorm2d): batchnorm(960, epsilon=1e-05)\n",
       "            (relu6): relu6\n",
       "          }\n",
       "          (conv2d): conv2d(960, 160, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "          (batchnorm2d): batchnorm(160, epsilon=1e-05)\n",
       "        }\n",
       "        (skip): skip\n",
       "      }\n",
       "      (add): add\n",
       "    }\n",
       "    (bottleneck_16): sequential('bottleneck') {\n",
       "      (conv_norm): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(160, 960, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "        (batchnorm2d): batchnorm(960, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv_norm_1): sequential('conv_norm') {\n",
       "        (conv2d): conv2d(960, 960, kernel=(3, 3), stride=(1, 1), padding=((1, 1), (1, 1)), bias=False)\n",
       "        (batchnorm2d): batchnorm(960, epsilon=1e-05)\n",
       "        (relu6): relu6\n",
       "      }\n",
       "      (conv2d): conv2d(960, 320, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(320, epsilon=1e-05)\n",
       "    }\n",
       "    (conv_norm): sequential('conv_norm') {\n",
       "      (conv2d): conv2d(320, 1280, kernel=(1, 1), stride=(1, 1), padding=((0, 0), (0, 0)), bias=False)\n",
       "      (batchnorm2d): batchnorm(1280, epsilon=1e-05)\n",
       "      (relu6): relu6\n",
       "    }\n",
       "  }\n",
       "  (mean): mean\n",
       "  (classifier): sequential('classifier') {\n",
       "    (dropout): dropout\n",
       "    (linear): linear(1280, 1000)\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mobilenetv2()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first time you run this tends to be slow because the functions are compiling. \n",
    "# Subsequent runs should be much faster\n",
    "a = xb.random.uniform((10, 3, 224, 224))\n",
    "xb.pack(a) >> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = xb.split(model >> xn.log_softmax(axis=0), xb.skip) >> xn.losses.nll_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xb.random.uniform((10, 3, 224, 224))\n",
    "b = xb.random.bernoulli((10, 1)).astype(xb.jnp.int32)\n",
    "\n",
    "xb.pack(a, b) >> loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xb.random.uniform((10, 3, 224, 224))\n",
    "xb.pack(a) >> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xb.random.uniform((10, 3, 224, 224))\n",
    "xb.pack(a) >> model.features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

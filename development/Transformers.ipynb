{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "remarkable-nicaragua",
   "metadata": {},
   "source": [
    "# Transformers Implementation\n",
    "\n",
    "Here I'm going to implement the transformer model from [Attention is All You Need](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import xaby as xb\n",
    "import xaby.nn as xn\n",
    "import xaby.random as xr\n",
    "\n",
    "from xaby import jnp\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax.ops import index, index_add, index_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-cement",
   "metadata": {},
   "source": [
    "First up we need an embedding function, really common for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_val = 0\n",
    "w = xb.random.normal((16, 20))\n",
    "jnp.concatenate([jnp.zeros((16, 1), dtype=jnp.float32) + padding_val, w], axis=1)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding(xb.Fn):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx: int=0, padding_val: float=0.):\n",
    "        def embedding(x: xb.ArrayList, params: dict) -> xb.ArrayList:\n",
    "            indices = x[0]\n",
    "            if jnp.dtype(indices).type not in {jnp.int8, jnp.int16, jnp.int32, jnp.int64}:\n",
    "                raise ValueError(\"Input array must have an integer dtype\")\n",
    "            \n",
    "            weights = params[\"weights\"]\n",
    "            pad = jnp.zeros((embedding_dim, 1), dtype=jnp.float32) + padding_val\n",
    "            padded_weights = jnp.concatenate([pad, weights], axis=1)\n",
    "            return xb.pack(padded_weights[:, indices].T)\n",
    "        \n",
    "        super().__init__(jax.jit(embedding), 1, 1, \"embedding\")\n",
    "        \n",
    "        self.params[\"weights\"] = xr.normal((embedding_dim, num_embeddings))\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"embedding({self.num_embeddings}, {self.embedding_dim})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = xb.array([2,4,9,1,3,0,0,0])\n",
    "xb.pack(a) >> embedding(20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-johnston",
   "metadata": {},
   "source": [
    "Next up is to implement the scaled dot-product attention. It's defined in the paper as\n",
    "\n",
    "$$ \n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "There's also an optional mask before the softmax. Since this doesn't require any parameters, I can implement it completely with (JAX) numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "@xb.fn\n",
    "def attention(queries: jnp.DeviceArray, keys: jnp.DeviceArray, values: jnp.DeviceArray, mask: jnp.DeviceArray):\n",
    "    \"\"\" Computes the masked scaled dot-product attention \"\"\"\n",
    "    \n",
    "    # alignment scores\n",
    "    scores = jnp.matmul(queries, keys.T) / jnp.sqrt(keys.shape[1])\n",
    "    \n",
    "    masked = mask * scores\n",
    "    scores = jnp.where(masked != 0, x=scores, y=jnp.ones_like(scores)*-jnp.inf)\n",
    "\n",
    "    # weights\n",
    "    weights = jax.nn.softmax(scores, axis=1)\n",
    "    \n",
    "    # If there are rows of all -inf before the softmax, the output will be rows of all nans. Convert those\n",
    "    # nans into 0s\n",
    "  \n",
    "    return jnp.nan_to_num(jnp.matmul(weights, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_steps = 10\n",
    "dec_steps = 9\n",
    "embedding_dim = 512\n",
    "\n",
    "q, k = xb.random.normal((dec_steps, embedding_dim)), xb.random.normal((enc_steps, embedding_dim))\n",
    "v, mask = k, jnp.ones((dec_steps, enc_steps))\n",
    "\n",
    "%timeit xb.pack(q, k, v, mask) >> attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_head(embedding_dim: int, model_dim: int):\n",
    "    \"\"\" Expects four arrays, [Queries, Keys, Values, Mask]\"\"\"\n",
    "    inputs = xb.split(xn.linear(embedding_dim, model_dim, bias=False), \n",
    "                      xn.linear(embedding_dim, model_dim, bias=False),\n",
    "                      xn.linear(embedding_dim, model_dim, bias=False),\n",
    "                      xb.skip)\n",
    "    \n",
    "    head = inputs >> attention\n",
    "    head.name = \"attention_head\"\n",
    "    return head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihead_attention(embedding_dim: int, model_dim: int, n_heads: int, masked=False):\n",
    "    \"\"\" Expects four arrays: queries, keys, values, mask \"\"\"\n",
    "    if model_dim % n_heads != 0:\n",
    "        raise ValueError(\"Model dimension must be evenly divisible by the number of heads\")\n",
    "    \n",
    "    heads = [attention_head(embedding_dim, model_dim // n_heads) for _ in range(n_heads)]\n",
    "    \n",
    "    attention = xb.parallel(*heads) >> xb.concatenate(axis=1) >> xn.linear(model_dim, model_dim, bias=0)\n",
    "    attention.name = \"multihead_attention\"\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = model_dim = 16\n",
    "n_heads = 4\n",
    "\n",
    "q, k = xb.random.normal((enc_steps, embedding_dim)), xb.random.normal((enc_steps, embedding_dim))\n",
    "v, mask = k, jnp.ones((enc_steps, enc_steps))\n",
    "\n",
    "xb.pack(q, k, v, mask) >> multihead_attention(embedding_dim, model_dim, n_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-johnson",
   "metadata": {},
   "source": [
    "So far I've only been developing this to work on one training example at a time, but typically you'd want to train on a batch of examples. Luckily, JAX makes it pretty easy to map a function over a batch dimension with `vmap`. Let's see if I can get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out using JAX to map the attention model over batches\n",
    "atten = multihead_attention(embedding_dim, 16, 4)\n",
    "\n",
    "# To get it to work, in_axes must match the data structures, so use xb.pack to create an ArrayList\n",
    "# must be the same shape as the input ArrayList\n",
    "atten.forward = jax.vmap(atten.forward, in_axes=(xb.pack(0,0,0,0), None))\n",
    "\n",
    "# Add batch dimension, batch size = 10\n",
    "q, k = xb.random.normal((10, dec_steps, embedding_dim)), xb.random.normal((10, enc_steps, embedding_dim))\n",
    "v, mask = k, jnp.ones((10, dec_steps, enc_steps))\n",
    "\n",
    "xb.pack(q, k, v, mask) >> atten >> xb.shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-receiver",
   "metadata": {},
   "source": [
    "It works ðŸ˜†. You can use `jax.pmap` instead of `jax.vmap` to run the batches in parallel over multiple GPUs. At some point, I'll build this into XABY to make it easy.\n",
    "\n",
    "Okay, multi-headed attention is done. Each sub-module uses [layer normalization](https://arxiv.org/abs/1607.06450) on the output. I haven't implemented that in XABY yet, so add it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layernorm(xb.Fn):\n",
    "    def __init__(self, normalized_shape: Union[Tuple[int], int], epsilon=1e-5, elementwise_affine=True):\n",
    "        \n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        \n",
    "        def layernorm_no_affine(x: xb.ArrayList, params: dict) -> xb.ArrayList:\n",
    "            inputs, = x\n",
    "            n_dims = len(inputs.shape)\n",
    "            norm_axes = tuple(range(n_dims - len(normalized_shape), n_dims))\n",
    "            \n",
    "            mean = inputs.mean(axis=norm_axes, keepdims=True)\n",
    "            var = jnp.mean((inputs - mean)**2, axis=norm_axes, keepdims=True)\n",
    "            normed = (inputs - mean) / jnp.sqrt(var + epsilon)\n",
    "            \n",
    "            return xb.pack(normed)\n",
    "        \n",
    "        def layernorm_affine(x: xb.ArrayList, params: dict) -> xb.ArrayList:\n",
    "            scale, bias = self.params[\"scale\"], self.params[\"bias\"]\n",
    "            normed, = layernorm_no_affine(x, params)\n",
    "            return xb.pack(normed * scale + bias)\n",
    "        \n",
    "        if elementwise_affine:\n",
    "            super().__init__(jax.jit(layernorm_affine), 1, 1, \"layernorm\")\n",
    "            self.params[\"scale\"] = jnp.ones(normalized_shape)\n",
    "            self.params[\"bias\"] = jnp.zeros(normalized_shape)\n",
    "        else:\n",
    "            super().__init__(jax.jit(layernorm_no_affine), 1, 1, \"layernorm\")\n",
    "            \n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"layernorm({self.normalized_shape}, elementwise_affine={self.elementwise_affine})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-lebanon",
   "metadata": {},
   "source": [
    "Finally we need to put together a mask for the attention operations. The mask is doing two things. Firstly it needs to tell the attention modules to ignore padding tokens. We can have padding in both the encoder and decoder sequences so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-candy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_mask(decoder: bool = False):\n",
    "    \"\"\" Returns a function that appends a mask for attention to an input ArrayList. \n",
    "        This function expects either one or two arrays: [queries] or [queries, keys].\n",
    "        \n",
    "        The default mask blocks the attention modules from using padding steps. It assumes the embedding\n",
    "        vector for a padding input is all zeros.\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        decoder: optional, bool, returns a mask that blocks pad embeddings and leftward flowing information\n",
    "        \n",
    "    \"\"\"\n",
    "    def append_mask(x: xb.ArrayList, p: dict) -> xb.ArrayList:\n",
    "        \"\"\" Creates a mask of all ones with shape [s_q, s_k] where s_q and s_k are the number of sequence\n",
    "            steps in the queries and keys, respectively, then adds it to the input ArrayList\n",
    "        \"\"\"\n",
    "        if len(x) == 1:\n",
    "            q, k = x[0], x[0]\n",
    "        else:\n",
    "            q, k = x\n",
    "\n",
    "        # Embeddings for padding tokens should be all zeros, so mask out where rows sum to 0\n",
    "        q_sums = (q.sum(axis=1) != 0).astype(jnp.int8)\n",
    "        k_sums = (k.sum(axis=1) != 0).astype(jnp.int8)\n",
    "        padding_mask = (q_sums * k_sums.reshape(-1, 1))\n",
    "\n",
    "        if decoder:\n",
    "            # The decoder needs to mask out illegal connections to prevent leftward flowing information\n",
    "            mask = jnp.tri(q.shape[0], k.shape[0]) * padding_mask\n",
    "        else:\n",
    "            mask = padding_mask\n",
    "        return xb.pack(*x, mask)\n",
    "    \n",
    "    return xb.Fn(jax.jit(append_mask), 3, 4, name=\"append_mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-cover",
   "metadata": {},
   "source": [
    "Now to finish up the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(func):\n",
    "    res = xb.parallel(func, xb.skip) >> xb.add\n",
    "    res.name = \"residual\"\n",
    "    return res\n",
    "\n",
    "def feedforward(model_dim, internal_dim):\n",
    "    linear1 = xn.linear(model_dim, internal_dim, bias=False)\n",
    "    linear2 = xn.linear(internal_dim, model_dim, bias=False)\n",
    "    ff = linear1 >> xn.relu >> linear2 \n",
    "    ff.name = \"feedforward\"\n",
    "    return ff\n",
    "\n",
    "def encoder_layer(embedding_dim: int, model_dim: int, n_heads: int, ff_dim: int, dropout: float=0.1) -> xb.Fn:\n",
    "    \"\"\" Encoder layer for a transformer model. Expects one input array with shape [n_steps, embedding_dim] \"\"\"\n",
    "    \n",
    "    multihead = multihead_attention(embedding_dim, model_dim, n_heads)\n",
    "    attn = xb.parallel(xb.select(0,0,0,1) >> multihead >> xn.dropout(dropout), xb.select(0)) >> xb.add >> layernorm(model_dim)\n",
    "    ff_layer = feedforward(model_dim, ff_dim) >> xn.dropout(dropout)\n",
    "    \n",
    "    layer = append_mask() >> attn >> residual(ff_layer) >> layernorm(model_dim)\n",
    "    layer.name = \"encoder_layer\"\n",
    "    \n",
    "    return layer\n",
    "\n",
    "def encoder(embedding_dim: int, model_dim: int, n_heads: int, ff_dim:int, n_layers: int, dropout: float=0.1) -> xb.Fn:\n",
    "    \"\"\" Encoder for a transformer model. Expects one input array with shape [n_steps, embedding_dim] \"\"\"\n",
    "    \n",
    "    layers = [encoder_layer(embedding_dim, model_dim, n_heads, ff_dim, dropout) for n in range(n_layers)]\n",
    "    \n",
    "    encoder = xb.sequential(*layers)\n",
    "    encoder.name = \"encoder\"\n",
    "    \n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_steps = 10\n",
    "embedding_dim = model_dim = 16\n",
    "ff_dim = 32\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "\n",
    "# Testing with padding tokens (0 by  default)\n",
    "tokens = xb.array([1,2,3,4,5,0,0,0])\n",
    "xb.pack(tokens) >> embedding(32, embedding_dim) >> encoder(embedding_dim, model_dim, n_heads, ff_dim, n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-welding",
   "metadata": {},
   "source": [
    "There we go, I have output from the encoder! Time for the decoder. A decoder layer is more complicated because there are two attention sub-layers. And we need different masks for both due to handling padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-application",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_layer(embedding_dim: int, model_dim: int, n_heads: int, ff_dim: int, dropout=0.1) -> xb.Fn:\n",
    "    \"\"\" Expects two input arrays: target embeddings and the encoder output. \"\"\"\n",
    "    \n",
    "    masked_attn = multihead_attention(embedding_dim, model_dim, n_heads)\n",
    "    attn = multihead_attention(embedding_dim, model_dim, n_heads)\n",
    "    \n",
    "    # Self-attention sub-layer. Select queries from input arrays [target embeddings, encoder output]\n",
    "    self_attn = append_mask(True) >> xb.select(0,0,0,1) >> masked_attn >> xn.dropout(dropout)\n",
    "    self_attn_layer = residual(self_attn) >> layernorm(model_dim)\n",
    "    # This returns a single array [self-attention]\n",
    "    \n",
    "    # Encoder attention sub-layer, this should accept two arrays [self-attention, encoder output]\n",
    "    enc_attn = append_mask() >> xb.select(0,1,1,2) >> attn >> xn.dropout(dropout)\n",
    "    enc_attn_layer = xb.parallel(enc_attn, xb.select(0)) >> xb.add >> layernorm(model_dim)\n",
    "    \n",
    "    # Feedforward sub-layer, expects one array from the encoder attn sub-layer\n",
    "    ff_layer = residual(feedforward(model_dim, ff_dim) >> xn.dropout(dropout)) >> layernorm(model_dim)\n",
    "    \n",
    "    # Input is two arrays, pass the first one to the self-attention layer, then combine the outputs\n",
    "    # and pass to the second attention sub-layer\n",
    "    layer = xb.split(self_attn_layer, xb.skip) >> enc_attn_layer >> ff_layer\n",
    "    layer.name = \"decoder_layer\"\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(embedding_dim: int, model_dim: int, n_heads: int, ff_dim:int, n_layers: int, dropout = 0.1) -> xb.Fn:\n",
    "    \"\"\" Decoder for a transformer model. Expects two input arrays: target embeddings and the encoder output. \"\"\"\n",
    "    \n",
    "    # We need to pass the encoder output to each layer, so use parallel and select to pass it along the layers\n",
    "    layers = [xb.parallel(decoder_layer(embedding_dim, model_dim, n_heads, ff_dim, dropout), xb.select(1))\n",
    "              for n in range(n_layers)]\n",
    "    \n",
    "    decoder = xb.sequential(*layers) >> xb.select(0)\n",
    "    decoder.name = \"decoder\"\n",
    "    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-injection",
   "metadata": {},
   "source": [
    "Okay, decoder is done. Now for the positional encodings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encoding(xb.Fn):\n",
    "    def __init__(self, embedding_dim: int, max_len=5000):\n",
    "        \n",
    "        pe = jnp.zeros((max_len, embedding_dim))\n",
    "        position = jnp.expand_dims(np.arange(0, max_len), 1)\n",
    "        div_term = jnp.exp(jnp.arange(0, embedding_dim, 2) * (-jnp.log(10000) / embedding_dim))\n",
    "        pe = index_update(pe, index[:, ::2], jnp.sin(position * div_term))\n",
    "        pe = index_update(pe, index[:, 1::2], jnp.cos(position * div_term))\n",
    "        \n",
    "        @jax.jit\n",
    "        def pos_encoding(x: xb.ArrayList, params: dict) -> xb.ArrayList:\n",
    "            embeddings, = x\n",
    "            n_steps = embeddings.shape[0]\n",
    "            encoded = embeddings + pe[:n_steps, :]\n",
    "            return xb.pack(encoded)\n",
    "        \n",
    "        super().__init__(pos_encoding, 1, 1, \"positional_encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-covering",
   "metadata": {},
   "source": [
    "Cool cool, now we have the parts ready to put the whole transformer model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(num_embeddings: int,\n",
    "                embedding_dim: int,\n",
    "                n_heads: int,\n",
    "                ff_dim: int,\n",
    "                encoder_layers: int,\n",
    "                decoder_layers: int,\n",
    "                dropout: float=0.1) -> xb.Fn:\n",
    "    \"\"\" Returns the transformer model which takes two arrays as input: [encoder inputs, decoder inputs]. \n",
    "        These inputs should be integer tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use one embedding for both inputs\n",
    "    embed = embedding(num_embeddings, embedding_dim)\n",
    "    pos_encode = positional_encoding(embedding_dim)\n",
    "    \n",
    "    encode = encoder(embedding_dim, embedding_dim, n_heads, ff_dim, encoder_layers, dropout=dropout)\n",
    "    decode = decoder(embedding_dim, embedding_dim, n_heads, ff_dim, decoder_layers, dropout=dropout)\n",
    "    \n",
    "    enc_input = embed >> pos_encode >> xn.dropout(dropout)\n",
    "    dec_input = embed >> pos_encode >> xn.dropout(dropout)\n",
    "    \n",
    "    probabilities = xn.linear(embedding_dim, num_embeddings) >> xn.softmax(axis=1)\n",
    "    \n",
    "    # Weight tying\n",
    "    embed.params[\"weights\"] = probabilities.linear.params[\"weights\"] * sqrt(embedding_dim) \n",
    "    \n",
    "    # Assuming we get two arrays as input: [encoding inputs, decoder inputs]\n",
    "    model = xb.split(enc_input >> encode, dec_input) >> decode >> probabilities\n",
    "    model.name = \"transformer\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting this to True will compile all the function control flow (sequential, parallel, etc) \n",
    "# in the model. Compilation takes much longer but the model runs 3x faster.\n",
    "xb.jit_combinators(False)\n",
    "\n",
    "model = transformer(100, 512, 8, 1024, 6, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-glass",
   "metadata": {},
   "source": [
    "Now we can test it out by passing in some test data. The function compiles the first time it's run, it can take a bit. But once that's done it runs fast. However, if the input shapes change, it needs to re-compile. This means in practice we would need to set a static size on the inputs and pad sequences that are shorter than this size. Since we're padding, we need to mask out padded entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = xb.array([1, 3, 3, 7, 5, 7, 0, 0, 0])\n",
    "targets = xb.array([12, 8, 10, 12, 11, 0, 0, 0, 0])\n",
    "\n",
    "# Model compiles on the first run, can take a bit\n",
    "xb.pack(source, targets) >> model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "source = xb.random.randint((9,), minval=0, maxval=20)\n",
    "targets = xb.random.randint((9,), minval=0, maxval=20)\n",
    "\n",
    "(xb.pack(source, targets) >> model)[0].block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-resort",
   "metadata": {},
   "source": [
    "Okay, again, let's see if we can get this operating over a batch of input data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward = jax.vmap(model.forward, in_axes=(xb.pack(0, 0), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = xb.array([[1, 3, 3, 7, 5, 7, 0, 0, 0],\n",
    "                   [2, 3, 3, 4, 5, 7, 2, 4, 0],\n",
    "                   [1, 3, 3, 7, 5, 7, 1, 0, 0],\n",
    "                   [1, 3, 3, 7, 5, 0, 0, 0, 0]])\n",
    "target = source * 2\n",
    "\n",
    "# This takes a bit to compile. Uncomment to run it\n",
    "# xb.pack(source, target) >> model >> xb.shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-think",
   "metadata": {},
   "source": [
    "Alright, next up is putting together the loss function and writing the training loop. The loss is a pretty straightforward cross-entropy loss. But, a few things I need to take care of here. The model needs the target tokens shifted over by one place, with a \"start-of-sentence\" token at the beginning. The loss needs the original tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class shift_targets(xb.Fn):\n",
    "    def __init__(self, start_token=1):\n",
    "        \n",
    "        @jax.jit\n",
    "        def shift(x: xb.ArrayList, params: dict) -> xb.ArrayList:\n",
    "            source, targets = x\n",
    "            \n",
    "            # Shift everything over one spot\n",
    "            # Typically I would do this with some in-place operations. But you have to use\n",
    "            # index_update with JAX for in-place things, and for some reason this causes \n",
    "            # an error when I vmap the top-level function\n",
    "            shifted = jnp.concatenate([jnp.array([start_token]), targets[:-1]])\n",
    "            \n",
    "            return xb.pack(source, shifted)\n",
    "        \n",
    "        super().__init__(shift, 2, 2, \"shift_targets\")\n",
    "\n",
    "def model_loss(model, smoothing=0.1):\n",
    "    \"\"\" Returns a function that expects two input arrays: [source tokens, target tokens] \"\"\"\n",
    "    loss = xb.parallel(shift_targets() >> model, xb.select(1)) >> xn.cross_entropy_loss(smoothing=smoothing)\n",
    "    loss = xb.set_meta(loss, name=\"model loss\", n_inputs=2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = xb.array([1, 3, 3, 7, 5, 7, 0, 0, 0])\n",
    "targets = xb.array([12, 8, 10, 12, 11, 0, 0, 0, 0])\n",
    "\n",
    "model = transformer(100, 512, 8, 1024, 6, 6)\n",
    "loss = model_loss(model)\n",
    "\n",
    "# This takes a bit to compile. Uncomment to run it\n",
    "xb.pack(source, targets) >> loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = xb.array([[1, 3, 3, 7, 5, 7, 0, 0, 0],\n",
    "                   [2, 3, 3, 4, 5, 7, 2, 4, 0],\n",
    "                   [1, 3, 3, 7, 5, 7, 1, 0, 0],\n",
    "                   [1, 3, 3, 7, 5, 0, 0, 0, 0]])\n",
    "targets = source * 2\n",
    "\n",
    "model = transformer(100, 512, 8, 1024, 6, 6)\n",
    "loss = model_loss(model)\n",
    "loss = xb.batchify(loss) >> xb.mean(axis=0)\n",
    "\n",
    "xb.pack(source, targets) >> loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harmful-establishment",
   "metadata": {},
   "source": [
    "# XABY is functional machine learning\n",
    "\n",
    "XABY is a machine learning library designed to be super readible and flexible. It's pronounced with a z/zed and [all caps](https://www.youtube.com/watch?v=gSJeHDlhYls) please.\n",
    "\n",
    "It's focused on assembling pure functions into one big pure function. It's functional! And, it provides a new way to develop machine learning models. PyTorch and Tensorflow 2.0 have converged on very similar APIs, so here's something different. XABY is built on top of JAX as well, the new cool kid in the machine learning world.\n",
    "\n",
    "Everything in XABY is a function that takes an ArrayList as input. Many functions return other functions. There are functions that collect functions together into one higher-level function. And all of these functions take ArrayLists.\n",
    "\n",
    "ArrayLists are the basic data structure in XABY, they're really just a slightly fancy Python list. ArrayLists collect JAX arrays and allow for passing data between functions in the most easy manner. There are also a lot of functions for manipulating ArrayLists to produce new ArrayLists.\n",
    "\n",
    "**Note:** XABY is very much a prototype. Expect names of things to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acquired-boxing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mat/miniconda3/envs/xaby/lib/python3.6/site-packages/jax/lib/xla_bridge.py:120: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import xaby as xb      # Base XABY things\n",
    "import xaby.nn as xn   # For neural networks ðŸ§ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-hostel",
   "metadata": {},
   "source": [
    "To start out, let's pack up some arrays. `xb.pack` returns an ArrayList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "false-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArrayList:\n",
      "DeviceArray([1., 2., 3.], dtype=float32)\n",
      "DeviceArray([3, 4, 5, 6], dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "arr_list = xb.pack(xb.array([1., 2, 3]), xb.array([3,4,5,6]))\n",
    "print(arr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-cleaner",
   "metadata": {},
   "source": [
    "This is just a list, you can append to it, index into it, iterate over it, etc. XABY provides a functions for working with ArrayLists. For example, `xb.select` allows you to select specific arrays from a list and return a new ArrayList with those arrays. Here's some fancy syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disabled-gross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArrayList:\n",
       "DeviceArray([1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_list >> xb.select(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-aquatic",
   "metadata": {},
   "source": [
    "What's happening here is that `xb.select(0)` returns a function that selects the first item in `arr_list`, as another ArrayList. Every function that would return arrays will always return those arrays packed into an ArrayList. \n",
    "\n",
    "Note that you can also call use `xb.select` like this but it's a bit more boring in my opinion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "flexible-contributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArrayList:\n",
       "DeviceArray([1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.select(0)(arr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-shore",
   "metadata": {},
   "source": [
    "`select` allows you to repeat arrays as well, it's super handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spoken-peninsula",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArrayList:\n",
       "DeviceArray([1., 2., 3.], dtype=float32)\n",
       "DeviceArray([3, 4, 5, 6], dtype=int32)\n",
       "DeviceArray([1., 2., 3.], dtype=float32)\n",
       "DeviceArray([3, 4, 5, 6], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_list >> xb.select(0, 1, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-heater",
   "metadata": {},
   "source": [
    "Why this notation with `>>`? Often when you're building deep learning models you get many nested functions. In normal Python, the first function executed is the most inner function, the last function executed is the outer-most. So when you read a common operation in a neural network like \n",
    "\n",
    "```python\n",
    "x = sigmoid(fc2(relu(fc1(x))))\n",
    "```\n",
    "\n",
    "you're reading left to right, from the last function to the first. To make this code more readible, I built XABY so functions are executed in the order they are written:\n",
    "\n",
    "```python\n",
    "x = x >> fc1 >> relu >> fc2 >> sigmoid\n",
    "```\n",
    "\n",
    "Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "difficult-finding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArrayList:\n",
       "DeviceArray(4., dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = xb.pack(xb.array([1., 2., 3, 4]),\n",
    "                 xb.array([3., 4, 5, 6]))\n",
    "\n",
    "\n",
    "# This is the mean squared error!\n",
    "inputs >> xb.sub >> xb.power(y=2) >> xb.mean(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-thousand",
   "metadata": {},
   "source": [
    "Let's step through this.\n",
    "- `xb.sub` expects an input with two arrays, subtracts them element-wise, returning an ArrayList with one array\n",
    "- `xb.power(y=2)` calculates $x^y$. It's actually a function that returns a function. So here, I called it with `y=2`. This returns another function that expects a single array (in an ArrayList) and returns each element raised to the power of 2.\n",
    "- `xb.mean(axis=None)` returns a function that calculates the mean of an array. It also expects only one array as input.\n",
    "\n",
    "In general, whenever you see a function that is called with some parameters, it will return another function that accepts an ArrayList. Functions that don't require extra parameters are called like `inputs >> xb.sub`.\n",
    "\n",
    "So, that's fun, yeah? You might like this then! We can build a reuseable mean squared error function like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acquired-murray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArrayList:\n",
       "DeviceArray(4., dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose multiple functions into one function\n",
    "mse = xb.sub >> xb.power(y=2) >> xb.mean(axis=None)\n",
    "\n",
    "inputs >> mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-money",
   "metadata": {},
   "source": [
    "As you see, `mse` is itself a function that accepts ArrayLists. Under the hood, each of the functions are called in order and the whole thing is compiled using JAX's just-in-time compiler. This makes it super fast! Much of what you do with XABY is compose functions from other functions and pass in an ArrayList with the expected inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-hungary",
   "metadata": {},
   "source": [
    "## What about neural networks?\n",
    "\n",
    "Deep learning (known to many as \"AI\") has produced amazing results over the last 6 or 7 years. I built XABY as a deep learning framework, so now I'll show you how to implement and train deep learning models. If you don't know much about deep learning, but want to understand what's going on here, check out [this free course](https://www.udacity.com/course/deep-learning-pytorch--ud188) I helped create at Udacity.\n",
    "\n",
    "Here's a super simple feedforward network with one hidden layer. It takes 10 input features and returns a binary classification probability for each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handled-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xn.linear(10, 5) >> xn.relu >> xn.linear(5, 1) >> xn.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-horizontal",
   "metadata": {},
   "source": [
    "That's it, that's the model. It's a function that accepts an ArrayList, it expects one array. Behind the scenes, `xn.linear` is initializing parameters that are passed to the forward functions. These parameters are collected by the composed function (a `sequential` function) in the `model.params` attribute. You call the model function like any other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subject-primary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArrayList:\n",
       "DeviceArray([[0.49970198],\n",
       "             [0.50403184],\n",
       "             [0.42720988],\n",
       "             [0.5106314 ],\n",
       "             [0.45148745],\n",
       "             [0.46111318],\n",
       "             [0.487898  ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a random 7x10 array, pack it up\n",
    "x = xb.pack(xb.random.uniform((7, 10)))\n",
    "\n",
    "# Forward pass through the model\n",
    "x >> model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-gambling",
   "metadata": {},
   "source": [
    "Cool, you built a model and can get predictions out. But for it to be useful, you need to train it. For this you need two things: a loss function to measure error and a way to update the parameters so the loss is minimized.\n",
    "\n",
    "Common loss functions are provided in `xaby.nn.losses`. The model returns probabilities for binary classification, so we should use the binary cross-entropy loss, `xaby.nn.losses.binary_cross_entropy_loss`. This is just another function like the others. It expects two inputs: an array of probabilities between 0 and 1, and an array of binary labels, 0 or 1. We need to compose a function that takes input data and returns the loss. The function should look something like this:\n",
    "\n",
    "![Binary cross entropy loss diagram](assets/loss_diagram.png)\n",
    "\n",
    "XABY has multiple functions for composing other functions in various configurations. Here is how you would compose a loss function using the model defined above and the `binary_cross_entropy_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "innovative-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces another function, it's all functions!\n",
    "loss = xb.split(model, xb.skip) >> xn.losses.binary_cross_entropy_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-swimming",
   "metadata": {},
   "source": [
    "Let's step through what's going on here:\n",
    "- `xb.split` splits the input ArrayList into individual arrays and maps the arrays to the given functions\n",
    "- `[features, targets] >> split(model, skip)` is equivalent to `[features >> model, targets >> skip]`\n",
    "- `xb.skip` simply returns the input ArrayList, it's a no-op.\n",
    "- `xb.split` packs the output of each function into an ArrayList\n",
    "\n",
    "Putting all this together, `xb.split(model, xb.skip)` creates a function that takes an ArrayList `[features, labels]` and returns an ArrayList `[probabilities, labels]`. Again, much of the work with XABY is composing functions like this.\n",
    "\n",
    "Now I'll make some fake data and calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "composed-charity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7117464542388916\n"
     ]
    }
   ],
   "source": [
    "# Make a random 7x10 array for features and another for targets\n",
    "features = xb.random.uniform((7, 10))\n",
    "targets = xb.random.bernoulli((7, 1))\n",
    "inputs = xb.pack(features, targets)\n",
    "\n",
    "# You can calculate the loss just like any other function\n",
    "print(f\"Loss: {inputs >> loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-archive",
   "metadata": {},
   "source": [
    "With the loss, you need a way to update the parameters. In comes backpropagation. The idea here is you get the gradients of all the model parameters with respect to the loss. Then we use these gradients to update the model parameters using stochastic gradient descent (SGD). Again, if you don't know what I'm talking about, check out [this deep learning course](https://www.udacity.com/course/deep-learning-pytorch--ud188). It's pretty straightforward to get the gradients with XABY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fallen-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradients and the loss for this batch\n",
    "batch_loss, grads = loss << inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-breach",
   "metadata": {},
   "source": [
    "This returns the loss and the gradients for all the function parameters. We can update the `loss` function parameters with the gradients using `xaby.optim.sgd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secret-career",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7115735411643982\n"
     ]
    }
   ],
   "source": [
    "# This returns a function\n",
    "update = xb.optim.sgd(lr=0.003)\n",
    "\n",
    "# Update the loss function parameters with the gradients\n",
    "update(loss, grads)\n",
    "\n",
    "# Calculate the loss again, it should be lower now\n",
    "print(f\"Loss: {inputs >> loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-uniform",
   "metadata": {},
   "source": [
    "Parameter updates are propagated to every sub-function so the model function is updated as well. You'd want to loop through a dataset in batches, updating the model for each batch. But this tutorial is long enough. I've created two more notebooks that show you how to train image classifiers on the MNIST dataset, [check them out!](https://github.com/mcleonard/xaby/tree/master/examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST with XABY\n",
    "\n",
    "This notebook demonstrates how to train a fully connected network (not convolutional!) on MNIST with the XABY framework. I'll also compare it to PyTorch so you can see the different APIs and performances.\n",
    "\n",
    "I'm going to use torchvision to load in the MNIST data, because it's super great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xaby as xb\n",
    "import xaby.nn as xn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"~/.pytorch\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(\"~/.pytorch\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models\n",
    "\n",
    "First up, I'll define two models with the same architecture. One with XABY, the other with PyTorch. \n",
    "\n",
    "XABY models are defined as a sequence of operations. When a model is defined, it is compiled behind the scenes into a single function. You call the function with some input like `inputs >> model`. I had a lot of fun messing with Python operators. My intention of doing it this way is if you chain a lot of functions, the last function called is the first function you read. I'm using the `>>` operator so you can write the chain of functions in the order they are called.\n",
    "\n",
    "You can define the PyTorch model with `torch.nn.Sequential`, but sublassing from `torch.nn.Module` is the preferred method, so I'll do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XABY model ##\n",
    "xaby_model = xb.flatten(axis=0) >> xn.linear(784, 256) >> xn.relu \\\n",
    "          >> xn.linear(256, 10) >> xn.log_softmax(axis=0)\n",
    "\n",
    "## PyTorch Model ##\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "torch_model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run data through the models!\n",
    "\n",
    "Just a small example of using XABY models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the image loader\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Convert PyTorch Tensor to a XABY array (actually a JAX DeviceArray)\n",
    "inputs = xb.array(images)\n",
    "\n",
    "# Thanks to JAX, XABY tensors are automatically on the GPU (if one is available)\n",
    "print(f\"XABY device: {inputs.device_buffer.device()}\")\n",
    "\n",
    "# # Call the model in a fun manner\n",
    "log_p = xb.pack(inputs) >> xaby_model\n",
    "\n",
    "# Normal function call... boring....\n",
    "log_p = xaby_model([inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should also note you can run XABY tensors through operations without creating models. This returns another tensor. If you start the sequence with an operation, it'll create a model. If you start with a tensor, it'll run through the operations and return a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing XABY and PyTorch\n",
    "\n",
    "Below I'll test how long it takes for inference with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First on CPU\n",
    "torch_model = torch_model.requires_grad_(False)\n",
    "torch_model.to(\"cpu\")\n",
    "images = images.to(\"cpu\")\n",
    "\n",
    "%timeit -n 1000 torch_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now on GPU\n",
    "torch_model.to(\"cuda\")\n",
    "images = images.to(\"cuda\")\n",
    "\n",
    "%timeit -n 1000 torch_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the XABY model, runs on GPU!\n",
    "inputs = xb.pack(xb.array(images))\n",
    "\n",
    "%timeit -n 1000 inputs >> xaby_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XABY is slightly slower than PyTorch on the GPU. This might be due to JAX being slower or it's possible I can do some more optimization in XABY.\n",
    "\n",
    "Either way, time to train the models. First up, XABY. I'll use simple stochastic gradient descent for both. The output of the models is log-softmax, so I'll use the negative log-likelihood loss.\n",
    "\n",
    "In XABY, we create a `backprop` function that takes the input and targets, then returns the loss and gradients. When only evaluating, such as in validation, you can get the loss directly:\n",
    "```python\n",
    "loss = inputs >> model >> nlloss << targets\n",
    "```\n",
    "\n",
    "We also create an `update` function that updates a model given gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a fresh model, in two lines for readability\n",
    "model = xb.flatten(axis=0) >> xn.linear(784, 256) >> xn.relu \\\n",
    "                           >> xn.linear(256, 10) >> xn.log_softmax(axis=0)\n",
    "\n",
    "# loss function\n",
    "loss = xb.nn.nll_loss(model)\n",
    "\n",
    "# Update function\n",
    "update = xb.optim.sgd(lr=0.003)\n",
    "\n",
    "step = 0\n",
    "start = time.time()\n",
    "for images, labels in train_loader:\n",
    "    step += 1\n",
    "    \n",
    "    # Wrap up our input data\n",
    "    inputs = xb.pack(xb.array(images), xb.array(labels))\n",
    "    \n",
    "    # Get the gradients. loss is just a function that accepts two arrays. It returns the mean loss\n",
    "    # for the batch and the gradients for the model parameters\n",
    "    train_loss, grads = inputs >> loss\n",
    "    \n",
    "    # Then, update the model with the gradients\n",
    "    update(model, grads)\n",
    "    \n",
    "    if step % print_every == 0:\n",
    "        stop = time.time()\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            inputs = xb.pack(xb.array(images), xb.array(labels))\n",
    "            \n",
    "            log_p, = model([inputs[0]])\n",
    "            pred_label = xb.jnp.argmax(log_p, axis=1)\n",
    "            test_accuracy.append((inputs[1] == pred_label).mean())\n",
    "            \n",
    "            test_loss, _ = inputs >> loss\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "        print(f\"Train loss: {train_loss:.3f}  \"\n",
    "              f\"Test loss: {sum(test_losses)/len(test_losses):.3f}  \"\n",
    "              f\"Test acc.: {sum(test_accuracy)/len(test_accuracy):.3f}  \"\n",
    "              f\"Images/sec: {print_every*batch_size/(stop - start):.3f}\")\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a fresh model\n",
    "torch_model = torch.nn.Sequential(\n",
    "                    torch.nn.Flatten(),\n",
    "                    torch.nn.Linear(784, 256),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(256, 10),\n",
    "                    torch.nn.LogSoftmax(1))\n",
    "torch_model.to(\"cpu\")\n",
    "optimizer = torch.optim.SGD(torch_model.parameters(), lr=0.003)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "step = 0\n",
    "start = time.time()\n",
    "for images, labels in train_loader:\n",
    "    step += 1\n",
    "    \n",
    "    inputs, targets = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    log_p = torch_model(inputs)\n",
    "    loss = criterion(log_p, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    if step % print_every == 0:\n",
    "        stop = time.time()\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        for images, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "                log_p = torch_model(inputs)\n",
    "                loss = criterion(log_p, targets)\n",
    "                accuracy = (log_p.argmax(axis=1) == targets).sum()/float(len(images))\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "            test_accuracy.append(accuracy.item())\n",
    "            \n",
    "        print(f\"Train loss: {train_loss:.3f}  \"\n",
    "              f\"Test loss: {sum(test_losses)/len(test_losses):.3f}  \"\n",
    "              f\"Test accuracy: {sum(test_accuracy)/len(test_accuracy):.3f}  \"\n",
    "              f\"Images/sec: {print_every*batch_size/(stop - start):.3f}\")\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

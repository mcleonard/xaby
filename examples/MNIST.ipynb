{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST with XABY\n",
    "\n",
    "This notebook demonstrates how to train a fully connected network (not convolutional!) on MNIST with the XABY framework. I'll also compare it to PyTorch so you can see the different APIs and performances.\n",
    "\n",
    "I'm going to use torchvision to load in the MNIST data, because it's super great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mat/miniconda3/envs/xaby/lib/python3.6/site-packages/jax/lib/xla_bridge.py:120: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import xaby as xb\n",
    "import xaby.nn as xn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"~/.pytorch\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(\"~/.pytorch\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models\n",
    "\n",
    "First up, I'll define two models with the same architecture. One with XABY, the other with PyTorch. \n",
    "\n",
    "XABY models are defined as a sequence of operations. When a model is defined, it is compiled behind the scenes into a single function. You call the function with some input like `inputs >> model`. I had a lot of fun messing with Python operators. My intention of doing it this way is if you chain a lot of functions, the last function called is the first function you read. I'm using the `>>` operator so you can write the chain of functions in the order they are called.\n",
    "\n",
    "You can define the PyTorch model with `torch.nn.Sequential`, but sublassing from `torch.nn.Module` is the preferred method, so I'll do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XABY model ##\n",
    "xaby_model = xb.flatten(axis=0) >> xn.linear(784, 256) >> xn.relu \\\n",
    "          >> xn.linear(256, 10) >> xn.log_softmax(axis=0)\n",
    "\n",
    "## PyTorch Model ##\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "torch_model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run data through the models!\n",
    "\n",
    "Just a small example of using XABY models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XABY device: CPU_0\n"
     ]
    }
   ],
   "source": [
    "# Get data from the image loader\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Convert PyTorch Tensor to a XABY array (actually a JAX DeviceArray)\n",
    "inputs = xb.array(images)\n",
    "\n",
    "# Thanks to JAX, XABY tensors are automatically on the GPU (if one is available)\n",
    "print(f\"XABY device: {inputs.device_buffer.device()}\")\n",
    "\n",
    "# # Call the model in a fun manner\n",
    "log_p = xb.pack(inputs) >> xaby_model\n",
    "\n",
    "# Normal function call... boring....\n",
    "log_p = xaby_model([inputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should also note you can run XABY tensors through operations without creating models. This returns another tensor. If you start the sequence with an operation, it'll create a model. If you start with a tensor, it'll run through the operations and return a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing XABY and PyTorch\n",
    "\n",
    "Below I'll test how long it takes for inference with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632 µs ± 160 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# First on CPU\n",
    "torch_model = torch_model.requires_grad_(False)\n",
    "torch_model.to(\"cpu\")\n",
    "images = images.to(\"cpu\")\n",
    "\n",
    "%timeit -n 1000 torch_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to time things on a GPU\n",
    "# torch_model.to(\"cuda\")\n",
    "# images = images.to(\"cuda\")\n",
    "\n",
    "# %timeit -n 1000 torch_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637 µs ± 42.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Now the XABY model, automatically runs on a GPU if one is available\n",
    "inputs = xb.pack(xb.array(images))\n",
    "\n",
    "%timeit -n 1000 inputs >> xaby_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XABY is slightly slower than PyTorch in this test. This might be due to JAX being slower or it's possible I can do some more optimization in XABY.\n",
    "\n",
    "Either way, time to train the models. First up, XABY. I'll use simple stochastic gradient descent for both. The output of the model is the log-probability, so I'll use the negative log-likelihood loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_loader.batch_size\n",
    "print_every = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 4.637  Test loss: 4.584  Test acc.: 0.513  Images/sec: 3541.424\n",
      "Train loss: 4.366  Test loss: 4.338  Test acc.: 0.687  Images/sec: 8872.720\n",
      "Train loss: 4.143  Test loss: 4.107  Test acc.: 0.746  Images/sec: 8731.576\n",
      "Train loss: 3.969  Test loss: 3.912  Test acc.: 0.778  Images/sec: 8306.100\n",
      "Train loss: 3.720  Test loss: 3.762  Test acc.: 0.802  Images/sec: 8700.962\n",
      "Train loss: 3.657  Test loss: 3.650  Test acc.: 0.820  Images/sec: 7908.449\n",
      "Train loss: 3.688  Test loss: 3.566  Test acc.: 0.831  Images/sec: 8166.549\n",
      "Train loss: 3.543  Test loss: 3.503  Test acc.: 0.839  Images/sec: 8024.907\n",
      "Train loss: 3.559  Test loss: 3.455  Test acc.: 0.849  Images/sec: 9572.297\n"
     ]
    }
   ],
   "source": [
    "### Define a fresh model, in two lines for readability\n",
    "model = xb.flatten(axis=0) >> xn.linear(784, 256) >> xn.relu \\\n",
    "                           >> xn.linear(256, 10) >> xn.log_softmax(axis=0)\n",
    "\n",
    "# loss function\n",
    "loss = xb.split(model, xb.skip) >> xn.losses.nll_loss()\n",
    "\n",
    "# Update function\n",
    "update = xb.optim.sgd(lr=0.003)\n",
    "\n",
    "step = 0\n",
    "start = time.time()\n",
    "for images, labels in train_loader:\n",
    "    step += 1\n",
    "    \n",
    "    # Wrap up our input data\n",
    "    inputs = xb.pack(xb.array(images), xb.array(labels))\n",
    "    \n",
    "    # Get the gradients\n",
    "    train_loss, grads = loss << inputs\n",
    "    \n",
    "    # Then, update the function with the gradients\n",
    "    update(loss, grads)\n",
    "    \n",
    "    if step % print_every == 0:\n",
    "\n",
    "        stop = time.time()\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            inputs = xb.pack(xb.array(images), xb.array(labels))\n",
    "            \n",
    "            log_p, = inputs >> xb.select(0) >> model\n",
    "            pred_label = xb.jnp.argmax(log_p, axis=1)\n",
    "            test_accuracy.append((inputs[1] == pred_label).mean())\n",
    "            \n",
    "            test_loss = inputs >> loss\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "        print(f\"Train loss: {train_loss:.3f}  \"\n",
    "              f\"Test loss: {sum(test_losses)/len(test_losses):.3f}  \"\n",
    "              f\"Test acc.: {sum(test_accuracy)/len(test_accuracy):.3f}  \"\n",
    "              f\"Images/sec: {print_every*batch_size/(stop - start):.3f}\")\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.090  Test loss: 2.047  Test accuracy: 0.489  Images/sec: 6658.876\n",
      "Train loss: 1.827  Test loss: 1.792  Test accuracy: 0.657  Images/sec: 7181.570\n",
      "Train loss: 1.590  Test loss: 1.551  Test accuracy: 0.727  Images/sec: 9431.559\n",
      "Train loss: 1.370  Test loss: 1.337  Test accuracy: 0.770  Images/sec: 8120.559\n",
      "Train loss: 1.254  Test loss: 1.159  Test accuracy: 0.795  Images/sec: 8477.698\n",
      "Train loss: 0.958  Test loss: 1.020  Test accuracy: 0.813  Images/sec: 7962.972\n",
      "Train loss: 0.891  Test loss: 0.911  Test accuracy: 0.826  Images/sec: 6625.582\n",
      "Train loss: 0.802  Test loss: 0.825  Test accuracy: 0.835  Images/sec: 6090.134\n",
      "Train loss: 0.733  Test loss: 0.757  Test accuracy: 0.844  Images/sec: 8422.017\n"
     ]
    }
   ],
   "source": [
    "# Start with a fresh model\n",
    "torch_model = torch.nn.Sequential(\n",
    "                    torch.nn.Flatten(),\n",
    "                    torch.nn.Linear(784, 256),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(256, 10),\n",
    "                    torch.nn.LogSoftmax(1))\n",
    "torch_model.to(\"cpu\")\n",
    "optimizer = torch.optim.SGD(torch_model.parameters(), lr=0.003)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "step = 0\n",
    "start = time.time()\n",
    "for images, labels in train_loader:\n",
    "    step += 1\n",
    "    \n",
    "    inputs, targets = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    log_p = torch_model(inputs)\n",
    "    loss = criterion(log_p, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    if step % print_every == 0:\n",
    "        stop = time.time()\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        for images, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "                log_p = torch_model(inputs)\n",
    "                loss = criterion(log_p, targets)\n",
    "                accuracy = (log_p.argmax(axis=1) == targets).sum()/float(len(images))\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "            test_accuracy.append(accuracy.item())\n",
    "            \n",
    "        print(f\"Train loss: {train_loss:.3f}  \"\n",
    "              f\"Test loss: {sum(test_losses)/len(test_losses):.3f}  \"\n",
    "              f\"Test accuracy: {sum(test_accuracy)/len(test_accuracy):.3f}  \"\n",
    "              f\"Images/sec: {print_every*batch_size/(stop - start):.3f}\")\n",
    "        start = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
